{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "56537a5b",
   "metadata": {},
   "source": [
    "# Chapter 1 Pengenalan Data\n",
    "\n",
    "## 1.0 Pengenalan Machine Learning\n",
    "\n",
    "<img src=\"https://i.ibb.co/brv8S5G/2.png\" width=\"500\"> \n",
    "<img src=\"https://i.ibb.co/92QPdKG/3.png\n",
    "\" width=\"500\">\n",
    "\n",
    "Istilah machine learning pertama kali dipopulerkan oleh Arthur Samuel, seorang ilmuwan komputer yang memelopori kecerdasan buatan pada tahun 1959. Menurutnya, machine learning adalah suatu cabang ilmu yang memberi komputer kemampuan untuk belajar tanpa diprogram secara eklpisit(gamblang).  \n",
    "Pemograman tradisional memiliki keterbatasan. Sifatnya rigid(kaku) dengan sekumpulan aturan \"if\" dan \"else\" untuk memproses data atau menyesuaikan dengan masukan. \n",
    "Hubungan antara AI, ML, dan deep learning. Machine Learning adalah cabang dari artificial intelligence. Kecerdasan buatan memiliki pengertian yang sangat luas tapi secara umum dapat dipahami sebagai komputer dengan kecerdasan layaknya manusia. Sedangkan ML memiliki arti lebih spesifik yaitu menggunakan metode statistika untuk membuat komputer dapat mempelajari pola pada data tanpa perlu diprogram secara eksplisit. Lebih lanjut, deep learning adalah cabang machine learning dengan algoritma jaringan syaraf tiruan yang dapat belajar dan beradaptasi terhadap sejumlah besar data. Algoritma ini memungkinkan mesin untuk melihat pola dari data yang tidak terstruktur atau data yang fiturnya tidak dapat ditentukan secara langsung. Contohnya, data gambar, teks, audio dan video.  \n",
    "\n",
    "<img src=\"https://i.ibb.co/NVvLwgK/4.png\" width=\"500\">\n",
    "\n",
    "## Mengapa Machine Learning\n",
    "\n",
    "Penggunaan pemograman tradisional memiliki dua kelemahan utama yaitu:\n",
    "1. Logika yang digunakan untuk membuat keputusan bersifat spesifik pada ranah dan masalah tertentu. Mengubah sedikit masalah, membuat kita mungkin perlu menulis keseluruhan sistem.\n",
    "2. Mendesain aturan untuk sistem memerluka pemahaman yang mendalam tentang bagaimana suatu keputusan harus dibuat oleh seorang ahli.\n",
    "\n",
    "## Jenis-Jenis Machine Learning\n",
    "\n",
    "Ada emoat kategori besar, yaitu supervised learning, unsupervised learning, semi-supervised learning, dan reiforcement learning. Pembagian kategori ini berdasarkan karakteristik data dan jenis supervsi yang dididapatkan oleh program selama pelatihan.\n",
    "1. Supervised Learning, adalah kategori machine learning yang menyertakan solusi yang diinginkan -yang disebut label- dalam proses pembelajarannya. Dataset yang digunakan telah memiliki label dan algoritma kemudian mempelajari pola dan pasangan data dan label tersebut. Algoritma supervised learning mudah dipahami dan performa akurasinya pun mudah diukur. Supervised learning dapat dilihat sebagai sebuah mesin/robot yang belajar menjawab pertanyaan sesuai dengan jawaban yang telah disediakan manusia.\n",
    "2. Unspervised Learning, dataset yang digunakan tidak memiliki label. Model unsupervised learning melakukan proses belajar sendiri untuk melabeli atau mengelompokkan data. Unspervised learning dapat dilihat sebagai robot/mesin yang berusaha belajar menjawab pertanyaan secara mandiri tanpa ada jawaban yang disediakan manusia.\n",
    "3. Semi-Supervised Learning, disini dataset untuk pelatihan sebagian memiliki label dan sebagian tidak. Google Photos adalah contoh implementasi yang sering kita gunakan. Pada Google Photos kita bisa memberi tag atau label untuk setiap orang yang ada dalam sebuah foto. Alhasil ketika kita menggugah foto baru dengan wajah orang yang sebelumnya kita beri label, Google Photos akan secara otomatis mengenali orang tersebut. Salah satu contoh dari model semi supervised learning adalah Deep Belied Network (DBNS). DBNS adalah model grafis dengan multipel layer yang dapat belajar teknik mengekstrak data training secara efisien. Dua jenis layer pada DBNS adalah visible atau input layer dan hidden layer. Menurut Geron, DBNS berdasar pada komponen unsupervised yang disebut restricted Boltzmann machine (RBMS). RBMS dilatih secara berurutan dengan algoritma unsupervised learning, kemudian seluruh sistem disesuaikan dengan teknik supervised learning. Campbell dalam tulisannya menyatakan bahwa pendekatan DBNS telah berhasil menyelesaikan pemodelan akustik pada speech recognition. DBNs menunjukkan sifat perkiraan yang kuat, peningkatan kinerja, dan merupakan parameter yang efisien. \n",
    "4. Reinforcement Learning, dikenal sebagai model yang belajar menggunakan sistem reward dan penalti. Menurut Winder, reinforcement learning adalah teknik yang mempelajari bagaimana membuat keputusan terbaik, secara berurutan, untuk memaksimalkan ukuran sukses kehidupan nyata. Entitas pembuat keputusan belajar melalui proses trial dan eror. Reinforcement learning memiliki empat komponen, yaitu action, agent, environment, dan reward. Action adalah setiap keputusan yang diambil. Misal, saat kita berkendara, action yang kita lakukan adalah mengendalikan kemudi, menginjak gas, dan mengerem. Agent adalah entitas yang membuat keputusan, contohnya adalah perangkat lunak, atau robot, atau bahkan manusia. Environment adalah sarana untuk berinteraksi, yang dapat menerima action dan memberikan respon berupa hasil maupun data berupa satu set observasi baru. Reward diberikan saat agent berhasil menyelesaikan tantangan. Mekanisme feedback ini membuat agent belajar tentang tindakan mana yang menyebabkan kesuksesan (menghasilkan reward), atau kegagalan (menghasilkan penalti). Keempat komponen tersebut merepresentasikan Markov decision process (MDP). Model reinforcement learning belajar agar terus mendapatkan reward dan menghindari penalti. AlphaGo, sebuah program yang dikembangkan oleh Google DeepMind adalah contoh terkenal dari reinforcement learning. AlphaGo dibuat untuk memainkan permainan Go, sebuah permainan papan kuno yang berasal dari Cina. AlphaGo mempelajari setiap langkah dalam jutaan permainan Go, untuk terus mendapatkan reward yaitu memenangkan permainan. AlphaGo terkenal setelah menjadi program komputer pertama yang berhasil mengalahkan seorang pemain Go profesional yang juga merupakan juara dunia.\n",
    "\n",
    "## Library Populer pada Python untuk Machine Learning dan Data Science\n",
    "\n",
    "1. Numpy. dikenal sebagai library untuk memproses larik atau array. Fungsi-fungsi kompleks di baliknya membuat Numpy sangat tangguh dalam memproses larik multidimensi dan matriks berukuran besar. Library ML seperti TensorFlow juga menggunakan Numpy untuk memproses tensor atau sebuah larik N dimensi.\n",
    "2. Pandas, menjadi library favorit untuk analisis dan manipulasi data. Kenapa keduanya penting? Sebelum masuk ke tahap pengembangan model, data perlu diproses dan dibersihkan. Proses ini bahkan merupakan proses yang paling banyak memakan waktu dalam pengembangan proyek ML. Library pandas membuat pemrosesan dan pembersihan data menjadi lebih mudah.\n",
    "3. Matplotlib adalah sebuah library untuk membuat plot atau visualisasi data dalam 2 dimensi. Matplotlib mampu menghasilkan grafik dengan kualitas tinggi. Matplotlib dapat dipakai untuk membuat plot seperti histogram, scatter plot, grafik batang, pie chart, hanya dengan beberapa baris kode. Library ini sangat ramah pengguna. \n",
    "4. Scikit Learn merupakan salah satu library ML yang sangat populer. Scikit Learn menyediakan banyak pilihan algoritma machine learning yang dapat langsung dipakai seperti klasifikasi, regresi, clustering, dimensionality reduction, dan pemrosesan data. Selain itu Scikit Learn juga dapat dipakai untuk analisis data.\n",
    "5. TensorFlow adalah framework open source untuk machine learning yang dikembangkan dan digunakan oleh Google. TensorFlow memudahkan pembuatan model ML bagi pemula maupun ahli. Ia dapat dipakai untuk deep learning, computer vision, pemrosesan bahasa alami (Natural Language Processing), serta reinforcement learning.\n",
    "6. Dikembangkan oleh Facebook, PyTorch adalah library yang dapat dipakai untuk masalah ML, computer vision, hingga pemrosesan bahasa alami. Bersaing dengan TensorFlow khususnya sebagai framework machine learning, PyTorch lebih populer di kalangan akademisi dibanding TensorFlow. Namun dalam industri, TensorFlow lebih populer karena skalabilitasnya lebih baik dibanding PyTorch.\n",
    "7. Keras adalah library deep learning yang luar biasa. Salah satu faktor yang membuat keras sangat populer adalah penggunaannya yang minimalis dan simpel dalam mengembangkan deep learning. Keras dibangun di atas TensorFlow yang menjadikan Keras sebagai API dengan level lebih tinggi (Higher level API) dari TensorFlow sehingga antarmukanya lebih mudah dari TensorFlow. Keras sangat cocok untuk mengembangkan model deep learning dengan waktu yang lebih singkat atau untuk pembuatan prototipe.\n",
    "\n",
    "## Data Collecting\n",
    "\n",
    "Ada tiga cara yang bisa kita lakukan untuk mengumpulkan data, yaitu.\n",
    "\n",
    "1. Mengekstrasi data (misal dari internet, riset, survei, dll).\n",
    "2. Mengumpulkan dan membuat dataset Anda sendiri dari nol.\n",
    "3. Menggunakan dataset yang telah ada.\n",
    "\n",
    "Beberapa sumber data internet yang dapat dimanfaatkan:\n",
    "1. UC Irvine Machine Learning Repository, UCI ML Repository adalah kumpulan database, teori, dan generator data yang digunakan oleh komunitas ML untuk analisis algoritma machine learning. Arsip tersebut awalnya dibuat sebagai arsip ftp pada tahun 1987 oleh David Aha, seorang mahasiswa pascasarjana UC Irvine. Sejak saat itu database UCI ML Repository ini digunakan secara luas oleh mahasiswa, staf pengajar, dan peneliti di seluruh dunia sebagai salah satu sumber utama dataset machine learning.\n",
    "2. Kaggle Dataset, Kaggle adalah komunitas belajar ilmu data paling populer di dunia. Kaggle memiliki peralatan dan sumber daya yang kuat untuk membantu kita belajar data science dan machine learning. Saat ini Kaggle memiliki 50.000 lebih publik dataset, baik dataset bersifat dummy ataupun riil yang dapat Anda unduh secara bebas.\n",
    "3. Google Dataset Search Engine, Pada akhir tahun 2018 Google meluncurkan Dataset Search, sebuah mesin pencari dataset. Tools ini bertujuan untuk menyatukan ribuan repositori dataset yang berbeda agar dataset tersebut lebih mudah ditemukan oleh pengguna.\n",
    "4. Tensorflow Dataset, Seperti yang telah dijelaskan pada sub-modul sebelumnya, TensorFlow adalah framework open source untuk machine learning yang dikembangkan dan digunakan oleh Google. Selain menyediakan learning resources, tensorflow juga menyediakan data resources yang cukup lengkap di library-nya mulai dari audio data, images, text, video, dan lainnya.\n",
    "5. US Government Data, Bagi Anda yang tertarik untuk mempelajari fenomena yang terjadi di Amerika Serikat, pemerintah Amerika meluncurkan data online resources yang mudah diakses oleh publik. Isinya antara lain data badai, data angka kelulusan dan dropout, data hewan-hewan yang terancam punah, statistik kriminal, dan berbagai data menarik lainnya.\n",
    "6. Satu Data Indonesia, Pemerintah Indonesia, melalui portal resmi Satu Data Indonesia menjalankan kebijakan tata kelola data pemerintah yang bertujuan untuk menciptakan data berkualitas, mudah diakses, dapat dibagi, dan digunakan oleh Instansi Pusat serta Daerah. Data dalam portal ini dapat diakses secara terbuka dan dikategorikan sebagai data publik, sehingga tidak memuat rahasia negara, rahasia pribadi, atau hal lain sejenisnya sebagaimana diatur dalam Undang-undang nomor 14 Tahun 2008 tentang Keterbukaan Informasi Publik.\n",
    "7. Open Data Pemerintah Jawa Barat, Open data Jawa Barat adalah portal resmi data terbuka milik Pemerintah Provinsi Jawa Barat yang berisikan data-data dari Perangkat Daerah di lingkungan Pemerintah Provinsi Jawa Barat. Open Data Jawa Barat ada untuk dapat memenuhi kebutuhan data publik bagi masyarakat. Data disajikan dengan akurat, akuntabel, valid, mudah diakses dan berkelanjutan. \n",
    "\n",
    "## Data Cleaning\n",
    "\n",
    "Padahal faktanya, seseorang yang bekerja di bidang data membutuhkan sebagian besar waktunya untuk melakukan proses data cleaning. Berikut adalah beberapa hal umum yang harus diperhatikan dalam proses data cleaning:\n",
    "1. Konsistensi Format, sebuah variabel mungkin tidak memiliki format yang konsisten seperti penulisan tanggal 10-Okt-2020 versus 10/10/20. Format jam yang berbeda seperti 17.10 versus 5.10 pm. Penulisan uang seperti 17000 versus Rp 17.000. Data dengan format berbeda tidak akan bisa diolah oleh model machine learning. Solusinya, format data harus disamakan dan dibuat konsisten terlebih dahulu.\n",
    "2. Skala Data, jika sebuah variabel memiliki jangka dari 1 sampai 100, pastikan tidak ada data yang lebih dari 100. Untuk data numerik, jika sebuah variabel merupakan bilangan positif, maka pastikan tidak ada bilangan negatif.\n",
    "3. Duplikasi data, data yang memiliki duplikat akan mempengaruhi model machine learning, apalagi jika data duplikat tersebut besar jumlahnya. Untuk itu kita harus memastikan tidak ada data yang terduplikasi.\n",
    "4. Missing Value, missing value terjadi ketika data dari sebuah record tidak lengkap. Missing value sangat mempengaruhi performa model machine learning. Ada 2 (dua) opsi untuk mengatasi missing value, yaitu menghilangkan data missing value atau mengganti nilai yang hilang dengan nilai lain, seperti rata-rata dari kolom tersebut (mean) atau nilai yang paling sering muncul (modus), atau nilai tengah (median).\n",
    "5. Skewness Distribution, skewness adalah kondisi di mana dataset cenderung memiliki distribusi data yang tidak seimbang. Skewness akan mempengaruhi data dengan menciptakan bias terhadap model. Apa itu bias? Sebuah model cenderung memprediksi sesuatu karena ia lebih sering mempelajari hal tersebut. Misalkan ada sebuah model untuk pengenalan buah di mana jumlah jeruk 92 buah dan apel 8 buah. Distribusi yang tidak imbang ini akan mengakibatkan model lebih cenderung memprediksi jeruk daripada apel. Cara paling simpel untuk mengatasi skewness adalah dengan menyamakan proporsi kelas mayoritas dengan kelas minoritas. \n",
    "\n",
    "## Data Processing\n",
    "\n",
    "Pandas dibangun menggunakan bahasa pemrograman Python yang menawarkan struktur data dan operasi untuk manipulasi tabel numerik dan time series. Tabel numerik adalah tabel yang berisi bilangan numerik dan Tabel time series adalah tabel yang berubah seiring waktu, misalnya tabel yang memuat perubahan nilai pasar saham untuk setiap menitnya. Dataframe adalah sebuah tabel yang terdiri dari kolom dan baris dengan banyak tipe data di dalamnya. Pandas terintegrasi dengan library machine learning yang populer seperti Scikit Learn (SKLearn) dan Numpy. Berikut adalah beberapa contoh data yang dapat diolah dengan pandas:\n",
    "1. CSV, adalah sebuah format data di mana elemen dari setiap baris dipisahkan dengan koma. CSV sendiri adalah singkatan dari Comma Separated Value.\n",
    "2. SQL, Structured Query Language adalah sebuah data yang berasal dari sebuah relational database. Format data ini berisi sebuah tabel yang memiliki format data seperti integer, string, float, dan biner.\n",
    "3. EXCEL, adalah berkas yang didapat dari spreadsheet seperti Microsoft Excel atau Google Spreadsheet. File Excel biasanya memuat data numerik.\n",
    "4. SPSS, SPSS atau Statistical Package for the Social Science adalah sebuah berkas dari perangkat lunak yang biasa dipakai untuk statistik dan pengolahan data. Berkas SPSS yang disimpan memiliki ekstensi .sav.\n",
    "5. JSON, JSON atau Javascript Object Notation adalah salah satu format data yang menggunakan sistem Key - Value di mana sebuah nilai disimpan dengan key tertetu untuk memudahkan mencari data.\n",
    "\n",
    "## Data Preparation dengan Teknik One-Hot-Encoding\n",
    "\n",
    "Biasanya, dataset Anda akan terdiri dari dua jenis data: kategorik dan numerik. Contoh data numerik adalah: ukuran panjang, suhu, nilai uang, hitungan dalam bentuk angka, dll, yang terdiri dari bilangan integer (seperti -1, 0, 1, 2, 3, dan seterusnya) atau bilangan float (seperti -1.0, 2.5, 39.99, dan seterusnya). Setiap nilai dari data dapat diasumsikan memiliki hubungan dengan data lain karena data numerik dapat dibandingkan dan memiliki ukuran yang jelas. Misal, Anda dapat mengatakan bahwa panjang 39 m lebih besar dibanding 21 m. Jenis data ini terdefinisi dengan baik, dapat dioperasikan dengan metode statistik, dan mudah dipahami oleh komputer. Data kategorik adalah data yang berupa kategori dan berjenis string, tidak dapat diukur atau didefinisikan dengan angka atau bilangan. Contoh data kategorik adalah sebuah kolom pada dataset yang berisi perkiraan cuaca seperti cerah, berawan, hujan, atau berkabut.  \n",
    "Umumnya, model machine learning tidak dapat mengolah data kategorik, sehingga kita perlu melakukan konversi data kategorik menjadi data numerik. Banyak model machine learning seperti Regresi Linear dan Support Vector Machine (kedua model ini akan dibahas pada modul-modul selanjutnya) yang hanya menerima input numerik sehingga tidak bisa memproses data kategorik. Salah satu teknik untuk mengubah data kategorik menjadi data numerik adalah dengan menggunakan One Hot Encoding atau yang juga dikenal sebagai dummy variables. One Hot Encoding mengubah data kategorik dengan membuat kolom baru untuk setiap kategori seperti gambar di bawah. \n",
    "\n",
    "## Data Preparation dengan Normalization dan Standardization\n",
    "\n",
    "Selain konversi data kategorik menjadi numerik, ada beberapa teknik lain dalam data preparation. Teknik yang akan dibahas antara lain membuang outlier, normalization, dan standardization.\n",
    "- Outlier Removal \n",
    "Dalam statistik, outlier adalah sebuah nilai yang jauh berbeda dari kumpulan nilai lainnya dan dapat mengacaukan hasil dari sebuah analisis statistik. Outlier dapat disebabkan oleh kesalahan dalam pengumpulan data atau nilai tersebut benar ada dan memang unik dari kumpulan nilai lainnya. Salah satu cara termudah untuk mengecek apakah terdapat outlier dalam data kita adalah dengan melakukan visualisasi. Dapat dilihat dengan jelas bahwa terdapat satu sampel yang jauh berbeda dengan sampel-sampel lainnya. Setelah mengetahui bahwa di data kita terdapat outlier, kita dapat mencari lalu menghapus sampel tersebut dari dataset.\n",
    "- Normalization \n",
    "Normalization adalah salah satu teknik yang dipakai dalam data preparation. Tujuan dari normalisasi adalah mengubah nilai-nilai dari sebuah fitur ke dalam skala yang sama. Normalization memungkinkan kenaikan performa dan stabilitas dari sebuah model machine learning. Contoh dari normalization adalah ketika kita memiliki dataset seperti di atas yang memiliki fitur umur dengan skala 23 sampai 45 tahun dan fitur penghasilan dengan skala 4.000.000 sampai 35.000.000. Di sini kita melihat bahwa fitur penghasilan sekitar satu juta kali lebih besar dari fitur umur dan menunjukkan kedua fitur ini berada pada skala yang sangat jauh berbeda.Ketika membangun model seperti regresi linear, fitur penghasilan akan sangat mempengaruhi prediksi dari model karena nilainya yang jauh lebih besar daripada umur, walaupun tidak berarti fitur tersebut jauh lebih penting dari fitur umur. Salah satu contoh dari normalization adalah min-max scaling di mana nilai-nilai dipetakan ke dalam skala 0 sampai 1. SKLearn menyediakan library untuk normalization. Pada Colab kita Import library MinMaxScaler dan masukkan data dari tabel sebelumnya.  \n",
    "` from sklearn.preprocessing import MinMaxScaler\n",
    "data = [[12000000, 33], [35000000, 45], [4000000, 23], [6500000, 26], [9000000, 29]] `  \n",
    "Pada cell selanjutnya kita buat sebuah objek MinMaxScaler dan panggil fungsi fit() dan mengisi argumen data seperti potongan kode di bawah. Fungsi fit() dari objek MinMaxSclaer adalah fungsi untuk menghitung nilai minimum dan maksimum pada tiap kolom.  \n",
    "` scaler = MinMaxScaler()\n",
    "scaler.fit(data) `  \n",
    "Sampai pada fungsi fit() ini, komputer baru menghitung nilai minimum dan maksimum pada tiap kolom dan belum melakukan operasi scaler pada data. Terakhir kita panggil fungsi transform() yang akan mengaplikasikan scaler pada data, sebagai berikut.  \n",
    "` print(scaler.transform(data)) `  \n",
    "- Standardization\n",
    "Standardization adalah proses konversi nilai-nilai dari suatu fitur sehingga nilai-nilai tersebut memiliki skala yang sama. Z score adalah metode paling populer untuk standardisasi di mana setiap nilai pada sebuah atribut numerik akan dikurangi dengan rata-rata dan dibagi dengan standar deviasi dari seluruh nilai pada sebuah kolom atribut. z = (value-mean) / standard deviation. Fungsi standardisasi itu serupa dengan normalization. Keduanya berfungsi menyamakan skala nilai dari tiap atribut pada data. SKLearn menyediakan library untuk mengaplikasikan standard scaler pada data. Pada colab di cell pertama kita akan mengimpor library preprocessing dari scikit learn lalu membuat data dummy sesuai dengan tabel di atas.  \n",
    "` from sklearn import preprocessing\n",
    "data = [[12000000, 33], [35000000, 45], [4000000, 23], [6500000, 26], [9000000, 29]] `  \n",
    "Selanjutnya kita buat object scaler dan panggil fungsi fit dari scaler pada data. Fungsi fit memiliki fungsi untuk menghitung rata-rata dan deviasi standar dari setiap kolom atribut untuk kemudian dipakai pada fungsi transform.  \n",
    "` scaler = preprocessing.StandardScaler().fit(data) `  \n",
    "Terakhir, kita panggil fungsi transform untuk mengaplikasikan standard scaler pada data. Untuk melihat hasil dari standard scaler kita tinggal memanggil objek scaler yang telah kita buat sebelumnya. Kodenya sebagai berikut.  \n",
    "` data = scaler.transform(data)\n",
    "data `  \n",
    "\n",
    "## Data Storage/Warehouse\n",
    "Data warehouse pertama kali muncul pada tahun akhir 1980-an. Tujuan awalnya adalah untuk membantu proses aliran data dari sistem operasional ke dalam sistem pendukung keputusan atau decision-support system (DSS). Seiring berjalannya waktu, data warehouse berkembang menjadi lebih efisien. Ia berevolusi dari penyimpanan informasi pendukung platform business intelligence menjadi infrastruktur analitis luas yang mendukung berbagai macam aplikasi.  \n",
    "AI dan machine learning telah mengubah banyak hal dari mulai industri, layanan, aset perusahaan, juga sistem data warehouse. Ekspansi big data dan penerapan teknologi digital mendorong perubahan dalam kapabilitas data warehouse. Untuk mendukung hal tersebut, ada beberapa tools yang perlu kita ketahui.\n",
    "1. RDBMS, Konsep Relational Database Management System (RDBMS) sendiri merupakan sistem yang mendukung adanya hubungan atau relasi antar tabel pada suatu database. Setiap tabel dihubungkan dengan tabel lainnya dengan menggunakan primary key dan foreign key. Saat ini sudah banyak jenis database yang menerapkan model RDBMS. Sebut saja MySQL, PostgreSQL, dan Microsoft SQL Server.\n",
    "2. NoSQL, Sesuai dengan namanya NoSQL adalah jenis basis data yang tidak menggunakan bahasa SQL dalam manipulasi datanya. Dalam penyimpanan datanya, NoSQL memiliki beberapa teknik penyimpanan yaitu: dokumen, graph, key-value, dan column based, antara lain. \n",
    "- Dokumen : Menghubungkan setiap kunci dengan struktur data kompleks yang disebut dokumen. \n",
    "- Graph : Menyimpan informasi tentang jaringan data, seperti koneksi sosial. \n",
    "- Nilai-kunci : Database NoSQL paling sederhana di mana setiap elemen dalam database disimpan sebagai nilai yang diasosiasikan dengan sebuah kunci. \n",
    "- Kolom : Menyimpan data yang memiliki volume besar, di mana setiap elemen data disimpan pada kolom bukan pada baris.\n",
    "Beberapa database NoSQL terpopuler adalah MongoDB, CouchDB, Cassandra, Redis, Neo4J, dan Riak. Jika ingin mengetahui lebih lanjut tentang NoSQL, kunjungi tautan berikut.\n",
    "3. Firebase Realtime Database, Sesuai namanya, “Database Realtime” adalah database yang menyimpan data yang berubah seiring waktu. Data jumlah penjualan harian, pengunjung mall setiap jam, arus lalu lintas setiap menit, atau fluktuasi saham setiap detik merupakan beberapa contoh data realtime. Data pada database realtime disimpan dalam format waktu dan nilai pada waktu yang terkait seperti gambar di bawah. Firebase Realtime Database (FRD) adalah database berbasis cloud yang didesain khusus untuk mengelola data realtime. FRD dapat menyimpan dan melakukan sinkronisasi data secara realtime di mana setiap kali ada perubahan data terbaru, FRD langsung menyimpannya pada Cloud. FRD juga dilengkapi fitur offline di mana ketika tidak ada koneksi internet, FRD akan menyimpan data secara lokal, kemudian saat online, akan melakukan sinkronisasi ke Cloud. Keren, kan?\n",
    "4. Spark, Apache Spark adalah perangkat lunak untuk pemrosesan dan analisis data berskala besar. Spark dapat digunakan dalam proses ETL (Extract, Transform, Load), data streaming, perhitungan grafik, SQL, dan machine learning. Untuk machine learning, Spark menyediakan MLlib yang berisi implementasi model machine learning seperti klasifikasi, regresi, pengklasteran, penurunan dimensi, dan pemfilteran kolaboratif.\n",
    "5. Big Query, BigQuery adalah data warehouse berbasis cloud untuk perusahaan yang menawarkan penyimpanan data berbasis SQL dan analisis data berukuran besar. Karena berbasis cloud dan tidak ada infrastruktur yang perlu dikelola, pengguna dapat berfokus pada pengolahan data tanpa memerlukan seorang administrator database.   \n",
    "Sebagai ML Engineer masa depan, kita harus mampu mengoperasikan berbagai jenis data storage dan data warehouse. Sebabnya, perusahaan tempat kita ingin bekerja sebagai ML Engineer nanti, tidak selalu menggunakan data warehouse atau data storage yang sama.\n",
    "\n",
    "## Datasets\n",
    "\n",
    "Pilihan yang lebih baik adalah dengan membagi dataset menjadi 2 bagian yaitu data training dan data testing. Data testing diambil dengan proporsi tertentu. Pada praktiknya, pembagian data training dan data testing yang paling umum adalah 80:20, 70:30, atau 60:40, tergantung dari ukuran atau jumlah data. Namun, untuk dataset berukuran besar, proporsi pembagian 90:10 atau 99:1 juga umum dilakukan. Misal jika ukuran dataset sangat besar berisi lebih dari 1 juta record, maka kita dapat mengambil sekitar 10 ribu data saja untuk testing alias sebesar 1% saja. Pada modul ini, kita akan belajar membagi dataset dengan fungsi train_test_split dari library sklearn. Perhatikan contoh kode berikut.  \n",
    "` from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1 ) `  \n",
    "Dengan fungsi train_test_split dari library sklearn, kita membagi array X dan y ke dalam 20% data testing (test_size=0.2 ). Misal total dataset A yang kita miliki adalah 1000 record, dengan test_size=0.2, maka data testing kita berjumlah 200 record dan jumlah data training sebesar 800 (80%). Sebelum proses pemisahan, fungsi train_test_split telah mengacak dataset secara internal terlebih dahulu. Jika tidak, data testing hanya akan berisi semua data pada kelas tertentu saja. Misal dataset A kita terdiri dari 5 kelas dengan jumlah masing-masing kelas sebesar 200 record, maka dengan proses shuffling sebelum pemisahan, data testing akan memiliki data dari 5 kelas yang ada. Tanpa proses shuffling, seluruh data dari kelas 1 - 4 akan berakhir di set data training, dan data testing hanya berisi data dari kelas 5 saja. Proses shuffling menjaga rasio informasi pada data training dan testing tetap berimbang. Melalui parameter random_state, fungsi train_test_split menyediakan random seed yang tetap untuk internal pseudo-random generator yang digunakan pada proses shuffling. Umumnya, nilai yang digunakan adalah 0, atau 1, atau ada juga yang menggunakan 42. Menentukan parameter random_state bertujuan untuk dapat memastikan bahwa hasil pembagian dataset konsisten dan memberikan data yang sama setiap kali model dijalankan. Jika tidak ditentukan, maka tiap kali melakukan split, kita akan mendapatkan data train dan tes berbeda, yang juga akan membuat akurasi model ML menjadi berbeda tiap kali di-run.  \n",
    "\n",
    "## Latihan SKLearn Train Test Split\n",
    "\n",
    "Untuk latihan membagi dataset terdiri dari tahapan-tahapan sebagai berikut:\n",
    "1. Persiapkan dataset ke dalam Notebook.\n",
    "2. Impor library SKLearn.\n",
    "3. Buat variabel untuk menampung data training dan data testing.\n",
    "4. Panggil fungsi train_test_split().\n",
    "Library sklearn menyediakan dataset iris yakni sebuah dataset yang umum digunakan untuk masalah klasifikasi. Dataset ini memiliki jumlah 150 sampel. Dataset iris dari library sklearn belum dapat langsung dipakai oleh sebuah model ML. Sesuai dengan yang telah dibahas pada modul terdahulu, kita harus memisahkan antara atribut dan label pada dataset. Untuk membuat train set dan test set kita tinggal memanggil fungsi train_test_split. Train_test_split memiliki parameter x yaitu atribut dari dataset, y yaitu target dari dataset, dan test_size yaitu persentase test set dari dataset utuh. Train_test_split mengembalikan 4 nilai yaitu, atribut dari train set, atribut dari test set, target dari train set, dan target dari test set. Ketika kita print panjang dari x_test, kita bisa melihat bahwa panjang dari atribut test set adalah 30 sampel, sesuai dengan parameter yang kita masukkan pada fungsi train_test_split yaitu 0.2 atau 20% dari 150 sampel.\n",
    "\n",
    "## Data Evaluation\n",
    "\n",
    "Sekarang bayangkan ketika kita bertugas untuk mengembangkan sebuah proyek ML. Kita bimbang kala memilih model yang akan dipakai dari 10 jenis model yang tersedia. Salah satu opsinya adalah dengan melatih semua model tersebut lalu membandingkan tingkat erornya pada test set. Setelah membandingkan kedua model, Anda mendapati model regresi linier memiliki tingkat eror yang paling kecil katakanlah sebesar 5%. Anda lalu membawa model tersebut ke tahap produksi. Kemudian ketika model diuji pada tahap produksi, tingkat eror ternyata sebesar 15%. Kenapa ini terjadi? Masalah ini disebabkan karena kita mengukur tingkat eror berulang kali pada test set. Kita secara tidak sadar telah memilih model yang hanya bekerja dengan baik pada test set tersebut. Hal ini menyebabkan model tidak bekerja dengan baik ketika menemui data baru. Solusi paling umum dari masalah ini adalah dengan menambahkan validation set pada model machine learning.  \n",
    "Validation set atau holdout validation adalah bagian dari train set yang dipakai untuk pengujian model pada tahap awal. Secara sederhana, kita menguji beberapa model dengan hyperparameter yang berbeda pada data training yang telah dikurangi data untuk validation. Lalu kita pilih model serta hyperparameter yang bekerja paling baik pada validation set. Setelah proses pengujian pada holdout validation, kita bisa melatih model menggunakan data training yang utuh (data training termasuk data validation) untuk mendapatkan model final. Terakhir kita mengevaluasi model final pada test set untuk melihat tingkat erornya. Dalam menggunakan holdout validation, ada beberapa hal yang harus dipertimbangkan. Jika ukuran validation set-nya terlalu kecil, maka ada kemungkinan kita memilih model yang tidak optimal. Sebaliknya, ketika ukurannya terlalu besar, maka sisa data pada train set lebih kecil dari data train set utuh. Kondisi ini tentu tidak ideal untuk membandingkan model yang berbeda pada data training yang lebih kecil. Solusi untuk masalah ini adalah dengan menggunakan Cross Validation.  \n",
    "K-Fold Cross Validation atau lebih sering disebut cross validation adalah salah satu teknik yang populer dipakai dalam evaluasi model ML. Pada cross validation dataset dibagi sebanyak K lipatan. Pada setiap iterasi setiap lipatan akan dipakai satu kali sebagai data uji dan lipatan sisanya dipakai sebagai data latih. Dengan menggunakan cross validation kita akan memperoleh hasil evaluasi yang lebih akurat karena model dievaluasi dengan seluruh data. Berikut adalah ilustrasi dari K-cross validation. \n",
    "\n",
    "## Latihan SKLearn Cross Validation Split\n",
    "\n",
    "Tahapan yang dilakukan pada codelab ini sebagai berikut:\n",
    "1. Impor library yang dibutuhkan.\n",
    "2. Pisahkan antara atribut dan label pada dataset.\n",
    "3. Buat model decision tree.\n",
    "4. Hitung hasil cross validation dari model dengan fungsi cross_val_score().  \n",
    "Kita akan membuat model machine learning pertama kita yaitu decision tree, menggunakan library scikit learn. Model machine learning juga sering disebut sebagai classifier. Lebih lanjut, variabel clf adalah singkatan dari classifier. Setelah dataset dan model siap, kita bisa menggunakan cross validation untuk mengevaluasi performa dari model machine learning. Fungsi cross_val_score() seperti di bawah menerima 4 parameter yaitu, ‘clf’ yang merupakan model machine learning, ‘X’ yang merupakan atribut dari dataset, ‘y’ yang merupakan label dari dataset, dan ‘cv’ yang merupakan jumlah fold yang akan dipakai pada cross validation. Cross_val_score mengembalikan nilai berupa larik atau array yang terdiri dari akurasi pengujian setiap fold dari dataset. Untuk mencetak dan mengetahui hasilnya, tambahkan kode scores di bawah kode sebelumnya. Tampilannya seperti gambar di bawah ini. Elemen pertama dari larik menunjukkan nilai 0.96666 yang berarti ketika fold pertama dijadikan validation set dan fold lainnya dijadikan train set, hasil dari pengujian tersebut adalah akurasi sebesar 0.96666. Melihat akurasi dari seluruh pengujian fold yang memiliki nilai tinggi dan konsisten pada tiap fold, kita mendapatkan gambaran bahwa model kita memiliki performa yang sangat baik. Secara umum jika hasil dari pengujian tiap fold pada cross validation memiliki nilai yang bervariasi dari 0.85 sampai 0.99, maka model tersebut dapat dikatakan baik."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
